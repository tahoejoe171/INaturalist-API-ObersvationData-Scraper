{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Observation Data Scraper for INaturalist API</h1>\n",
    "\n",
    "This notebook contains a scraper for retrieving data from the [iNaturalist API](https://www.inaturalist.org/pages/api+reference). The scraper collects observations based on user-specified parameters, such as species names, location, or media types (photos, sounds, etc.).\n",
    "\n",
    "This script utilizes the \"window\" method of requesting data from the API. Due to INaturalist's limit of 10,000 observations per request, you must first tailor your parameters to fetch no more than 10,000 results per each request. You should not alter the request rate to fetch more than 60 times per minute, and should also not request more than 10,000 times per day (though difficult). Download capacity (monitored by the program) is capped at 5GB per hour, and no more than 24GB per day per INaturalist's usage policy. If you start and stop this program rather than running it once per day, be sure to monitor your total data usage manually. Should an error occur, your data will automatically be saved.\n",
    "\n",
    "This program works to scrape together data through incremental search \"windows\" of all possible id numbers, compiling and formatting the data as necessary into a .csv format. It has a timer and data meter to prevent exceeding the download limit policy, and thus can be run in the background over the course of however long it will take. For example, processing 700k observations may take several days. Multiple IP's/proxies/VPN's can allow you to do this much faster, however that is a violation of the usage policy. \n",
    "\n",
    "Note: INaturalist's API is not intended for data scraping. While this script complies with their policies, I cannot condone its usage. I created this program for educational and proof of concept purposes. This script is especially intended to retrieve information that is unavailable through their formal data export tool, and can be used to collect more data than their data export tool allows (given enough time). Be sure to include appropriate attributions when using photos copyrighted under the Creative Commons License.<br>\n",
    "<a href=\"https://www.inaturalist.org/pages/api+recommended+practices\">INaturalist's API recommended practices</a> <br>\n",
    "<a href=\"https://www.inaturalist.org/observations/export\">INaturalist's formal data export tool</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Features:</h2>\n",
    "<ul>\n",
    "<p>-Fetches observations from Inaturalist based on configurable parameters</p>\n",
    "<p>-Filters observation data according to a configurable field list</p>\n",
    "<p>-Handles data issues of the currently pressent fields</p>\n",
    "<p>-Fully complies with Inaturalist's API use and data downloading policy</p>\n",
    "<p>-Saves results to CSV for further analysis</p>\n",
    "</ul>\n",
    "\n",
    "<H2>Prerequisites:</h2>\n",
    "<ul>\n",
    "<p>-Python 3.12 and the following libraries:</p>\n",
    "    <ul>\n",
    "        <p>-requests</p>\n",
    "        <p>-pandas</p>\n",
    "        <p>-time</p>\n",
    "        <p>-json</p>\n",
    "        <p>-math</p>\n",
    "        <p>-os</p>\n",
    "        <p>-datetime</p>\n",
    "    </ul>\n",
    "</ul>\n",
    "<h3>Scroll down to set up your parameters to begin</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--Click Run after prerequisite libraries are installed\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step one: Define your search parameters and personal details, excluding \"id_above\" and \"id_below\".</h3>\n",
    "Execute Get_Latest_ID() (below) before executing this block.\n",
    "You may also add or remove any parameters as you desire. For a complete list of observation query parameters, see <a href=\"https://www.inaturalist.org/pages/api%2Breference#get-observations\">the full documentation.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change these request headers and Parameters as needed. Add or remove fields in the getInfo() function below as desired. \n",
    "\n",
    "request_headers={   #change in step 1 to your own request headers (recommended). This is a generic headers sample.\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"application/json, text/html;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "Your_File_Path = r'C:\\YourFilePath'+'\\\\' #Change in step 1 (the r string only). filepath to folder of this notebook\n",
    "url=\"https://api.inaturalist.org/v1/observations\"\n",
    "ids_per_window=100000    #change in step 2\n",
    "try: #No need to change\n",
    "    total_ids = get_latest_ID()\n",
    "except NameError:\n",
    "    total_ids = 260916966\n",
    "number_of_windows=math.ceil(total_ids/ids_per_window)\n",
    "initial_window_number=0   #(As Necessary) Change this value to start a search at that window number. In case your execution is cut short, \n",
    "#you may resume at the next initial_window_number, following the last value in the last saved csv file name. The initial search always begins at 0.\n",
    "per_page=200 #results per page request (maximum allowed=200)\n",
    "\n",
    "\n",
    "the_parameters = {   #change in step 1\n",
    "    \"place_id\": 11, #Use the codeblock below to decide which place_id to use, if any\n",
    "    \"per_page\":per_page, \n",
    "    \"created_before\":\"2025-02-04\", #Keep in mind that the observation date could be long before the date it was posted to INaturalist (creation date)\n",
    "    \"id_above\":total_ids-(initial_window_number+1)*ids_per_window, \n",
    "    \"id_below\":total_ids-(initial_window_number)*ids_per_window,\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(Get_Place_ID('new york')) #Enter your desired location. Examples: Hawaii, USA, San Diego, Earth, Australia, new york city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step Two:</h3>\n",
    "Execute the large code block that starts with Get_Info() after Step 3. Then,\n",
    " run the following print function and note the total results. As long as that number is below 10,000, this program will work. If it exceeds 10,000, decrease the 'ids_per_window' in the_parameters above such that the total results are somewhere around 8,000 to be safe. If it is far below 10,000, increase 'ids_per_window' by a magnitude or two to make the search more efficient. The total results pulled from the API based on your search parameters may vary depending on the date/window, but the search windows with the higher ID number ranges (the most recent observations) will generally yield the highest total results.\n",
    "\n",
    "This program is currently set up to search through all possible INaturalist ID's. If you would like to only get data from a certain time period, say all data before 2020, then add the parameter \"created before\":\"YYYY-MM-DD\" to the function \"Get_Latest_ID()\" below, and execute that cell as well as the one containing the parameters. This will reduce unnecessary requests that fall outside of your date range. \n",
    "\n",
    "You can also open the testdata.json file that saves into the program folder to see the total_results as well as a sample of the raw data returned by the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Get_Total_Results())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step Three: Choose your target fields</h3>\n",
    "You can find a complete list of return fields <a href=\"https://www.inaturalist.org/pages/api+reference#get-observations\">here</a>.\n",
    "You may add or remove them from Get_Info() as necessary, following the current format, and they will be added into the final csv. In future updates, a feature to make this easier may be added.\n",
    "Be sure to match the order of the data_columns with the Get_Info() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes only the desired datapoints from each observation uploaded to INaturalist\n",
    "def Get_Info(observation):\n",
    "    extensions = ['.jpg','.jpeg','.png','.webp','.gif']\n",
    "    result=[] \n",
    "    if(observation['captive']==True): #only returns wild specimen observations\n",
    "        return\n",
    "    try:\n",
    "        result.append(observation['taxon']['name']) #Most specific taxon name given. Observation is voided altogether if there is no name.\n",
    "    except(KeyError, TypeError):\n",
    "        return None\n",
    "    try:\n",
    "        result.append(observation['id']) #Observation ID number\n",
    "    except (KeyError, TypeError):\n",
    "        result.append('None')\n",
    "    try:\n",
    "        result.append(observation['location']) #Location\n",
    "    except (KeyError, TypeError):\n",
    "        result.append('None')\n",
    "    try:\n",
    "        result.append(observation['positional_accuracy']) #Accuracy, in meters, of the location\n",
    "    except (KeyError, TypeError):\n",
    "        result.append('None')\n",
    "    try:\n",
    "        result.append(observation['observed_on']) #date of observation\n",
    "    except (KeyError, TypeError):\n",
    "        result.append('None')\n",
    "    try:\n",
    "        result.append(observation['species_guess']) \n",
    "    except (KeyError, TypeError):\n",
    "        result.append('None')\n",
    "    try:\n",
    "        result.append(observation['taxon']['rank']) #rank of the taxon (family, genus, order, etc.)\n",
    "    except (KeyError, TypeError):\n",
    "        result.append('None')\n",
    "    try:\n",
    "        result.append(observation['taxon']['preferred_common_name']) #preferred common name of the specimen. May be in a native language or different than the common name\n",
    "    except (KeyError, TypeError):\n",
    "        result.append('None')\n",
    "    try: #Retrieve the original photo url, accounting for both jpg and jpeg files. If not found, this observation is voided altogether\n",
    "        photo_url=observation['observation_photos'][0]['photo']['url']\n",
    "        matched = False\n",
    "        for ext in extensions:\n",
    "            if photo_url.lower().endswith(ext):\n",
    "                result.append(photo_url[:-(len(ext)+6)]+'original'+ ext)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            return None\n",
    "    except (KeyError, TypeError, IndexError):\n",
    "        return None\n",
    "    try: #Creative commons phoho copyright attribution\n",
    "        result.append((observation['observation_photos'][0]['photo']['attribution'])) #Copyright attribution for fair use of the original photo\n",
    "    except (KeyError, TypeError, IndexError):\n",
    "        result.append('None')\n",
    "    try:\n",
    "        result.append(observation['taxon']['conservation_status']['status_name']) #conservation status. ex:threatened, vulnerable, etc.\n",
    "    except (KeyError, TypeError):\n",
    "        result.append('None')\n",
    "    try: #Native, Introduced, or Endemic\n",
    "        result.append(observation['taxon']['establishment_means']['establishment_means']) #Known Establishment means. ex: native, endemic, introduced, none, etc. \n",
    "    except (KeyError, TypeError):\n",
    "        result.append('None')\n",
    "    try:\n",
    "        result.append(observation['obscured']) #exact loation may be obscured by several hundred or thousand meters, due to user privacy or taxon privacy(in the case of rare or endangered species)\n",
    "    except (KeyError, TypeError):\n",
    "        result.append('None')\n",
    "    try:\n",
    "        result.append(observation['taxon']['extinct']) #Extinct or not (Boolean)\n",
    "    except (KeyError, TypeError):\n",
    "        result.append('None') \n",
    "    return result   \n",
    "\n",
    "data_columns = ['taxon_name','id','location','positional_accuracy','observed_on','species_guess','taxon_rank','preferred_common_name',\n",
    "                'photo_url','copyright','conservation_status','establishment_means','location_obscured','rank_extinct'] #csv column names\n",
    "\n",
    "\n",
    "\n",
    "def Merge_Files(file_1_name, file_2_name):\n",
    "    file_1=file_1_name\n",
    "    file_2=file_2_name\n",
    "    alpha = pd.read_csv(f\"{Your_File_Path}{file_1}\")\n",
    "    beta=pd.read_csv(f\"{Your_File_Path}\\\\{file_2}\")\n",
    "    pd.concat([alpha,beta], ignore_index=False, axis=0, sort=False).drop_duplicates().to_csv(\"All_Data.csv\",index=False)\n",
    "\n",
    "\n",
    "def Get_Total_Results(): #returns the total results from the_parameters in the average search window\n",
    "    check_response = requests.get(url, params=the_parameters, headers=request_headers)\n",
    "    if check_response.status_code == 200: #response from API recieved\n",
    "        test_data = check_response.json()\n",
    "        with open(\"testdata.json\", \"w\") as json_file:\n",
    "            json.dump(test_data, json_file, indent=4)\n",
    "        return (test_data[\"total_results\"])\n",
    "\n",
    "#retrieve the ID of the most recent observation (according to the date parameters) uploaded to INaturalist\n",
    "def Get_Latest_ID():\n",
    "    parameters = {\n",
    "    \"per_page\":1,\n",
    "    #\"created_before\":\"YYYY-MM-DD\"\n",
    "    }\n",
    "    check_response = requests.get(url, params=parameters, headers=request_headers)\n",
    "    if check_response.status_code == 200: #response from API recieved\n",
    "        test_data = check_response.json()\n",
    "        return int(test_data[\"results\"][0]['id'])\n",
    "    \n",
    "#Retrieve the proper location ID \n",
    "def Get_Place_ID(place):\n",
    "    places_url = \"https://api.inaturalist.org/v1/places/autocomplete\"\n",
    "    params = {\"q\": f\"{place}\"}\n",
    "    response = requests.get(places_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        for place in data[\"results\"]:\n",
    "            print(f\"Place Name: {place['display_name']} - Place ID: {place['id']}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "\n",
    "#returns amount of hours until n hours from the start time, along with a boolean representing whether or not the current time has reached that time.\n",
    "def CompareTime(start_time, n):\n",
    "    hrs_ago=datetime.now()-timedelta(hours=n) \n",
    "    diff=(start_time-hrs_ago)\n",
    "    if diff.seconds/3600<=1:\n",
    "        return [False,abs(diff.seconds)] #returns the time until the next hour interval\n",
    "    else:\n",
    "        print(f'it has been {min(abs(diff.seconds/3600), abs(24-diff.seconds/3600))} hours since {n} hours from the start time, and 5GB has not been reached. Resetting hourly data consumption meter.')\n",
    "        return [True]\n",
    "    \n",
    "#Retrieves the raw data from the API and saves it to a .csv\n",
    "def Get_Data():\n",
    "    init_num=initial_window_number\n",
    "    download_size = 0 #ID window current download size\n",
    "    total_download_size = 0 #total daily download size\n",
    "    hrs=1 #Hours (1-6) counter\n",
    "    days=0 #days counter\n",
    "    all_data=pd.DataFrame(columns=data_columns)\n",
    "    start_time=datetime.now()\n",
    "    try:\n",
    "        while True:\n",
    "            while initial_window_number<=number_of_windows: #cycle through each window of ID's \n",
    "                page=1\n",
    "                page_limit=1\n",
    "                while (page<=(page_limit)): #cycle through each page of that window\n",
    "                    the_parameters[\"page\"]=page\n",
    "                    response = requests.get(url, params=the_parameters, headers=request_headers)\n",
    "                    if response.status_code == 200: #response from API recieved\n",
    "                        data = response.json()\n",
    "                        with open(\"data.json\", \"w\") as json_file:\n",
    "                            json.dump(data, json_file, indent=4)\n",
    "                        page_limit = (math.ceil(data['total_results']/per_page)) #Max amount of pages in that window\n",
    "                        for observation in data['results']:\n",
    "                            if not (x:= Get_Info(observation)) == None: #voids the observations that are missing important data points, which return None in getInfo()\n",
    "                                if (len(x) == len(data_columns)): #in case any columns are mismatched somehow\n",
    "                                    all_data.loc[len(all_data)]=x\n",
    "                    else: #response from API not received\n",
    "                        print(f\"Error: {response.status_code}\",'\\n', response.text)\n",
    "                        all_data.to_csv(f'all_data(failed){init_num}-{initial_window_number-1}outof{number_of_windows}.csv', index_label=\"index\") \n",
    "                        raise SystemExit(\"Stopping execution\")\n",
    "\n",
    "                    download_size+= (s:=os.path.getsize(f\"{Your_File_Path}data.json\")/1024/1024/1024) #count each page request size in GB\n",
    "                    total_download_size +=s\n",
    "                    print(f\"\\rDownloaded data this hour: {download_size:.2f} GB. \", end=\"\") #download size live update\n",
    "                    page+=1\n",
    "                    if(download_size)>4.85: #if more than 4.9 gigs of data have been downloaded and an hour hasnt passed, pause until the next hour begins. \n",
    "                        all_data.to_csv(f'alldata_windows_hr{hrs}_{init_num}_{initial_window_number-1}out_of{number_of_windows}.csv', index_label=\"index\") #save once per hour block.\n",
    "                        if CompareTime(start_time,hrs)[0]==False:\n",
    "                            download_size = 0\n",
    "                            print(f'{CompareTime(start_time,hrs)[1]//60} minutes until the next 5GB cycle.')\n",
    "                            time.sleep(CompareTime(start_time,hrs)[1]) #wait until the next hour interval, then continue\n",
    "                            hrs+=1\n",
    "                    if ((datetime.now()-start_time).seconds/3600)>=(hrs+1): #resetting the data counter and the time clock if an hour passes before 5gb is downloaded\n",
    "                            hrs+=1\n",
    "                            download_size = 0 \n",
    "                    if((total_download_size-days*23.85)>=23.85): #daily download threshold is reached.\n",
    "                        print(f\"Max Daily Data reached. Window {initial_window_number} incomplete. Continuing in 19 hours.\")\n",
    "                        download_size=0\n",
    "                        time.sleep(68400)\n",
    "                    time.sleep(1) #Maximum allowed request rate is 1 per second\n",
    "                print(f'\\rWindow {initial_window_number} Complete: {(initial_window_number+1)/number_of_windows:.2%}')\n",
    "                initial_window_number+=1 #Increment to the next \"window\" of ID's \n",
    "                the_parameters[\"id_below\"]=max(1257723,260757723-(initial_window_number)*1300000)\n",
    "                the_parameters[\"id_above\"] = max(0, 260757723 - ((initial_window_number + 1) * 1300000))\n",
    "            all_data.to_csv(f'all_data_windows{init_num}-{initial_window_number-1}/{number_of_windows}.csv', index_label=\"index\") #save final csv file once the end is reached.\n",
    "            raise SystemExit(\"Data Collection Complete.\")\n",
    "    except (KeyboardInterrupt, BaseException): #saves progress in case of an error of any kind\n",
    "        all_data.to_csv(f\"all_data_windows(failed){init_num}-{max(init_num,initial_window_number-1)}outof{number_of_windows}.csv\", index_label=\"index\")\n",
    "        print('Error. Processed data has been saved.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 4: Run the program</h3>\n",
    "Run the following code block. Your data will be saved into csv files named after the range of search windows it represents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Get_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 5: Compile</h3>\n",
    "Your data may result in multiple csv files depending on the amount you download or the amount of interruptions. They will need to be merged together without dupicates. Use the following code with the proper file names to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merge_Files(\"File1.csv\",\"File2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
